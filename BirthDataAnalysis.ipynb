{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaca4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Birth Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "table_path = \"hdfs:///user/abdur_zeybey/data_table_1920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f4dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cols = [\n",
    "            \"DOB_YY\", #Year\n",
    "            \"DOB_MM\", #Month\n",
    "            \"DOB_TT\",#Time\n",
    "            \"DOB_WK\",#Weekday\n",
    "            \"MAGER\", #Mother's Age\n",
    "            \"MEDUC\", #Mother’s Education\n",
    "            \"FAGECOMB\", #Father’s Age\n",
    "            \"FEDUC\",  #Father’s Education\n",
    "            \"CIG_0\", #Cigarettes Before Pregnancy\n",
    "            \"RF_PDIAB\", #Pre-pregnancy Diabetes\n",
    "            \"RF_PHYPE\", #Pre-pregnancy Hypertension\n",
    "            \"RF_INFTR\", #Infertility Treatment Used\n",
    "            \"RF_FEDRG\", #Fertility Enhancing Drugs\n",
    "            \"DPLURAL\", #Plurality Recode\n",
    "            \"SEX\", #Sex of Infant\n",
    "            \"DBWT\", #Birth Weight – Detail in Grams\n",
    "            \"AB_AVEN1\", #Assisted Ventilation (immediately)\t \n",
    "            \"AB_SEIZ\", #Seizures\t\n",
    "        ]\n",
    "\n",
    "cols = [\n",
    "            \"DOB_MM\", #Month\n",
    "            \"MAGER\", #Mother's Age\n",
    "            \"FAGECOMB\", #Father’s Age\n",
    "            \"PRIORLIVE\", #Prior Births Now Living\n",
    "            \"PRIORDEAD\", #Prior Births Now Dead\n",
    "            \"ILOP_R\", #Interval Since Last Other Pregnancy Recode\n",
    "            \"CIG_0\", #Cigarettes Before Pregnancy\n",
    "            \"BMI\", #Mother Body Mass Index\n",
    "            \"RF_PDIAB\", #Pre-pregnancy Diabetes\n",
    "            \"RF_PHYPE\", #Pre-pregnancy Hypertension\n",
    "            \"DPLURAL\", #Plurality Recode\n",
    "            \"SEX\", #Sex of Infant\n",
    "            \"AB_AVEN1\", #Assisted Ventilation (immediately)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5f4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_table = spark.read.parquet(table_path).select(cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fe12a",
   "metadata": {},
   "source": [
    "# Data Selection and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b099915d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of DataFrame[DOB_MM: int, MAGER: string, FAGECOMB: string, PRIORLIVE: int, PRIORDEAD: int, ILOP_R: int, CIG_0: int, BMI: string, RF_PDIAB: string, RF_PHYPE: string, DPLURAL: string, SEX: string, AB_AVEN1: string]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce41aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = data_table.filter(data_table.DOB_MM == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020c0707",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = data_table.drop(\"DOB_MM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa883d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|AB_AVEN1| count|\n",
      "+--------+------+\n",
      "|       Y| 30254|\n",
      "|       U|   443|\n",
      "|       N|586517|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = data_table.groupBy(\"AB_AVEN1\").count()\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a6cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(MAGER='25', FAGECOMB='29', PRIORLIVE=1, PRIORDEAD=0, ILOP_R=888, CIG_0=0, BMI='22.7', RF_PDIAB='N', RF_PHYPE='N', DPLURAL='1', SEX='M', AB_AVEN1='N')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d7a53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove null and Unknown values of Assisted Ventilation column\n",
    "data_table = data_table.na.drop(subset=[\"AB_AVEN1\"])\n",
    "data_table = data_table.filter(data_table.AB_AVEN1 != 'U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d586ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|AB_AVEN1| count|\n",
      "+--------+------+\n",
      "|       Y| 30254|\n",
      "|       N|586517|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res = data_table.groupBy(\"AB_AVEN1\").count()\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4712b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Unknown values of Pre-pregnancy Diabetes and Hypertension columns\n",
    "data_table = data_table.filter(data_table.RF_PDIAB != 'U')\n",
    "data_table = data_table.filter(data_table.RF_PHYPE != 'U')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96c1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Convert \"Y\" and \"N\" columns to numerical values (1 for \"Y\" and 0 for \"N\")\n",
    "yn_columns = [\"RF_PDIAB\", \"RF_PHYPE\",\"AB_AVEN1\"] # Replace with your \"Y\" and \"N\" column names\n",
    "for col_name in yn_columns:\n",
    "    data_table = data_table.withColumn(col_name, when(col(col_name) == 'N', 0).otherwise(1))\n",
    "\n",
    "data_table = data_table.withColumn(\"SEX\", when(col(\"SEX\") == 'M', 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f619dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(MAGER='25', FAGECOMB='29', PRIORLIVE=1, PRIORDEAD=0, ILOP_R=888, CIG_0=0, BMI='22.7', RF_PDIAB=0, RF_PHYPE=0, DPLURAL='1', SEX=0, AB_AVEN1=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1228b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# Replace 'int_col' with the name of the column containing integer values as strings\n",
    "data_table = data_table.withColumn('MAGER', col('MAGER').cast(IntegerType()))\n",
    "data_table = data_table.withColumn('FAGECOMB', col('FAGECOMB').cast(IntegerType()))\n",
    "data_table = data_table.withColumn('DPLURAL', col('DPLURAL').cast(IntegerType()))\n",
    "\n",
    "# If you have a column containing float values stored as strings, you can cast them to double\n",
    "# Replace 'float_col' with the name of the column containing float values as strings\n",
    "data_table = data_table.withColumn('BMI', col('BMI').cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "406e93cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(MAGER=25, FAGECOMB=29, PRIORLIVE=1, PRIORDEAD=0, ILOP_R=888, CIG_0=0, BMI=22.7, RF_PDIAB=0, RF_PHYPE=0, DPLURAL=1, SEX=0, AB_AVEN1=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3c2345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = data_table.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deae5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616382, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f572e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae41b3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616382, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c371bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               MAGER       FAGECOMB      PRIORLIVE      PRIORDEAD  \\\n",
      "count  616382.000000  616382.000000  616382.000000  616382.000000   \n",
      "mean       29.058548      39.864767       1.283770       0.248898   \n",
      "std         5.865198      22.731872       3.932441       4.803890   \n",
      "min        12.000000      10.000000       0.000000       0.000000   \n",
      "25%        25.000000      28.000000       0.000000       0.000000   \n",
      "50%        29.000000      33.000000       1.000000       0.000000   \n",
      "75%        33.000000      39.000000       2.000000       0.000000   \n",
      "max        50.000000      99.000000      99.000000      99.000000   \n",
      "\n",
      "              ILOP_R          CIG_0            BMI       RF_PDIAB  \\\n",
      "count  616382.000000  616382.000000  616382.000000  616382.000000   \n",
      "mean      744.656895       1.543457      28.957594       0.010140   \n",
      "std       332.641749       8.383737      12.512235       0.100185   \n",
      "min         3.000000       0.000000      13.000000       0.000000   \n",
      "25%       888.000000       0.000000      22.500000       0.000000   \n",
      "50%       888.000000       0.000000      26.000000       0.000000   \n",
      "75%       888.000000       0.000000      31.500000       0.000000   \n",
      "max       999.000000      99.000000      99.900000       1.000000   \n",
      "\n",
      "            RF_PHYPE        DPLURAL            SEX       AB_AVEN1  \n",
      "count  616382.000000  616382.000000  616382.000000  616382.000000  \n",
      "mean        0.022551       1.033708       0.489085       0.049051  \n",
      "std         0.148467       0.185355       0.499881       0.215974  \n",
      "min         0.000000       1.000000       0.000000       0.000000  \n",
      "25%         0.000000       1.000000       0.000000       0.000000  \n",
      "50%         0.000000       1.000000       0.000000       0.000000  \n",
      "75%         0.000000       1.000000       1.000000       0.000000  \n",
      "max         1.000000       5.000000       1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "summary = df.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92db6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('AB_AVEN1', axis=1)\n",
    "y = df['AB_AVEN1']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213cae73",
   "metadata": {},
   "source": [
    "# Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e61b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "svc = SVC()\n",
    "rand_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cde0a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the utility function\n",
    "def print_performance_metric(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa51ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.39      0.00      0.01      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.67      0.50      0.49    123277\n",
      "weighted avg       0.93      0.95      0.93    123277\n",
      "\n",
      "[[117378     27]\n",
      " [  5855     17]]\n"
     ]
    }
   ],
   "source": [
    "# Baseline performance\n",
    "print_performance_metric(log_reg, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57f50f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    117405\n",
      "           1       0.16      0.03      0.05      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.56      0.51      0.51    123277\n",
      "weighted avg       0.92      0.95      0.93    123277\n",
      "\n",
      "[[116455    950]\n",
      " [  5694    178]]\n"
     ]
    }
   ],
   "source": [
    "print_performance_metric(rand_forest, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a805dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines (SVM) with the default Radial basis function (RBF) kernel can be computationally expensive, \n",
    "#especially when dealing with large datasets or high-dimensional data. \n",
    "#print_performance_metric(svc, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843855d",
   "metadata": {},
   "source": [
    "# Experiment 1 - Scaling Continuous Features\n",
    "\n",
    "Row(MAGER=25, FAGECOMB=29, PRIORLIVE=1, PRIORDEAD=0, ILOP_R=888, CIG_0=0, BMI=22.7, RF_PDIAB=0, RF_PHYPE=0, DPLURAL=1, SEX=0, AB_AVEN1=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b66c8c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAGER</th>\n",
       "      <th>FAGECOMB</th>\n",
       "      <th>PRIORLIVE</th>\n",
       "      <th>PRIORDEAD</th>\n",
       "      <th>ILOP_R</th>\n",
       "      <th>CIG_0</th>\n",
       "      <th>BMI</th>\n",
       "      <th>RF_PDIAB</th>\n",
       "      <th>RF_PHYPE</th>\n",
       "      <th>DPLURAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AB_AVEN1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.691972</td>\n",
       "      <td>-0.477953</td>\n",
       "      <td>-0.072161</td>\n",
       "      <td>-0.051812</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>-0.184101</td>\n",
       "      <td>-0.500118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.524494</td>\n",
       "      <td>-0.038042</td>\n",
       "      <td>-0.326457</td>\n",
       "      <td>-0.051812</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>-0.184101</td>\n",
       "      <td>-0.835790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160515</td>\n",
       "      <td>2.601426</td>\n",
       "      <td>0.182134</td>\n",
       "      <td>-0.051812</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>1.008685</td>\n",
       "      <td>-0.683939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.521474</td>\n",
       "      <td>2.601426</td>\n",
       "      <td>0.182134</td>\n",
       "      <td>-0.051812</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>-0.184101</td>\n",
       "      <td>-0.572048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.544459</td>\n",
       "      <td>-0.873874</td>\n",
       "      <td>-0.326457</td>\n",
       "      <td>-0.051812</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>-0.184101</td>\n",
       "      <td>-0.635986</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MAGER  FAGECOMB  PRIORLIVE  PRIORDEAD    ILOP_R     CIG_0       BMI  \\\n",
       "0 -0.691972 -0.477953  -0.072161  -0.051812  0.430924 -0.184101 -0.500118   \n",
       "1  1.524494 -0.038042  -0.326457  -0.051812  0.430924 -0.184101 -0.835790   \n",
       "2  0.160515  2.601426   0.182134  -0.051812  0.430924  1.008685 -0.683939   \n",
       "3 -0.521474  2.601426   0.182134  -0.051812  0.430924 -0.184101 -0.572048   \n",
       "4 -1.544459 -0.873874  -0.326457  -0.051812  0.430924 -0.184101 -0.635986   \n",
       "\n",
       "   RF_PDIAB  RF_PHYPE  DPLURAL  SEX  AB_AVEN1  \n",
       "0         0         0        1    0         0  \n",
       "1         0         0        1    0         0  \n",
       "2         0         0        1    1         0  \n",
       "3         0         0        1    0         0  \n",
       "4         0         0        1    1         1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify continuous features to scale\n",
    "continuous_features = ['MAGER', 'FAGECOMB', 'BMI', 'PRIORLIVE', 'PRIORDEAD', 'ILOP_R', 'CIG_0']\n",
    "\n",
    "# Scale the continuous features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[continuous_features])\n",
    "\n",
    "# Replace original features with the scaled ones\n",
    "scaled_df = df.copy()\n",
    "scaled_df[continuous_features] = scaled_features\n",
    "\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eeb937ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with scaled features:\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.45      0.00      0.01      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.70      0.50      0.49    123277\n",
      "weighted avg       0.93      0.95      0.93    123277\n",
      "\n",
      "[[117387     18]\n",
      " [  5857     15]]\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    117405\n",
      "           1       0.17      0.03      0.05      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.56      0.51      0.51    123277\n",
      "weighted avg       0.92      0.95      0.93    123277\n",
      "\n",
      "[[116470    935]\n",
      " [  5687    185]]\n"
     ]
    }
   ],
   "source": [
    "X = scaled_df.drop('AB_AVEN1', axis=1)\n",
    "y = scaled_df['AB_AVEN1']\n",
    "\n",
    "X_train_2, X_test_2, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Retrain and evaluate the models using 2nd order polynomial features\n",
    "print(\"Performance with scaled features:\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print_performance_metric(log_reg, X_train_2, y_train, X_test_2, y_test)\n",
    "print(\"\\nRandom Forest:\")\n",
    "print_performance_metric(rand_forest, X_train_2, y_train, X_test_2, y_test)\n",
    "#print(\"\\nSupport Vector Machine:\")\n",
    "#print_performance_metric(svc, X_train_2, y_train, X_test_2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e609ad",
   "metadata": {},
   "source": [
    "# Experiment 2 - Polynomial Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba8a5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with 2nd order polynomial features:\n",
      "\n",
      "Logistic Regression:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.50      0.00      0.00      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.73      0.50      0.49    123277\n",
      "weighted avg       0.93      0.95      0.93    123277\n",
      "\n",
      "[[117397      8]\n",
      " [  5864      8]]\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    117405\n",
      "           1       0.17      0.03      0.05      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.56      0.51      0.51    123277\n",
      "weighted avg       0.92      0.95      0.93    123277\n",
      "\n",
      "[[116488    917]\n",
      " [  5681    191]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create PolynomialFeatures objects for 2nd order features\n",
    "poly_2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Generate the polynomial features\n",
    "X_poly_2 = poly_2.fit_transform(scaled_df.drop('AB_AVEN1', axis=1))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_3, X_test_3, y_train, y_test = train_test_split(X_poly_2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Retrain and evaluate the models using 2nd order polynomial features\n",
    "print(\"Performance with 2nd order polynomial features:\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print_performance_metric(log_reg, X_train_3, y_train, X_test_3, y_test)\n",
    "print(\"\\nRandom Forest:\")\n",
    "print_performance_metric(rand_forest, X_train_3, y_train, X_test_3, y_test)\n",
    "#print(\"\\nSupport Vector Machine:\")\n",
    "#print_performance_metric(svc, X_train_2, y_train, X_test_2, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700792ce",
   "metadata": {},
   "source": [
    "# Experiment 3 - Dimensinatility Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aeebd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Apply PCA and LDA transformations\n",
    "pca = PCA(n_components=0.95)\n",
    "lda = LDA()\n",
    "\n",
    "X_pca = pca.fit_transform(scaled_df.drop('AB_AVEN1', axis=1))\n",
    "X_lda = lda.fit_transform(scaled_df.drop('AB_AVEN1', axis=1), y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "X_train_lda, X_test_lda, y_train_lda, y_test_lda = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c003887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with PCA transformed features:\n",
      "\n",
      "Logistic Regression:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.00      0.00      0.00      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.48      0.50      0.49    123277\n",
      "weighted avg       0.91      0.95      0.93    123277\n",
      "\n",
      "[[117405      0]\n",
      " [  5872      0]]\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    117405\n",
      "           1       0.10      0.02      0.03      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.53      0.50      0.50    123277\n",
      "weighted avg       0.91      0.95      0.93    123277\n",
      "\n",
      "[[116554    851]\n",
      " [  5773     99]]\n",
      "\n",
      "\n",
      "Performance with LDA transformed features:\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.41      0.00      0.01      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.68      0.50      0.49    123277\n",
      "weighted avg       0.93      0.95      0.93    123277\n",
      "\n",
      "[[117382     23]\n",
      " [  5856     16]]\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96    117405\n",
      "           1       0.07      0.06      0.07      5872\n",
      "\n",
      "    accuracy                           0.92    123277\n",
      "   macro avg       0.51      0.51      0.51    123277\n",
      "weighted avg       0.91      0.92      0.92    123277\n",
      "\n",
      "[[113085   4320]\n",
      " [  5524    348]]\n"
     ]
    }
   ],
   "source": [
    "# Retrain and evaluate the models using PCA transformed features\n",
    "print(\"Performance with PCA transformed features:\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print_performance_metric(log_reg, X_train_pca, y_train_pca, X_test_pca, y_test_pca)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print_performance_metric(rand_forest, X_train_pca, y_train_pca, X_test_pca, y_test_pca)\n",
    "#print(\"\\nSupport Vector Machine:\")\n",
    "#print_performance_metric(svc, X_train_pca, y_train, X_test_pca, y_test)\n",
    "\n",
    "# Retrain and evaluate the models using LDA transformed features\n",
    "print(\"\\n\\nPerformance with LDA transformed features:\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print_performance_metric(log_reg, X_train_lda, y_train_lda, X_test_lda, y_test_lda)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print_performance_metric(rand_forest, X_train_lda, y_train_lda, X_test_lda, y_test_lda)\n",
    "#print(\"\\nSupport Vector Machine:\")\n",
    "#print_performance_metric(svc, X_train_lda, y_train, X_test_lda, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b4f52",
   "metadata": {},
   "source": [
    "# GridSearch for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c4c3343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 81 candidates, totalling 324 fits\n",
      "Best parameters found: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.62      0.01      0.02      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.79      0.51      0.50    123277\n",
      "weighted avg       0.94      0.95      0.93    123277\n",
      "\n",
      "[[117360     45]\n",
      " [  5799     73]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the RandomForestClassifier\n",
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rand_forest, param_grid=param_grid,\n",
    "                           cv=4, verbose=1, n_jobs=-1, scoring=\"recall\")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rand_forest = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "print_performance_metric(best_rand_forest, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867e327",
   "metadata": {},
   "source": [
    "# Creating Pipeline\n",
    "\n",
    "LDA transformed features and Random Forest with parameters {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} look like best model for the Birth Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b4416e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9525783398363036\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    117405\n",
      "           1       0.61      0.01      0.02      5872\n",
      "\n",
      "    accuracy                           0.95    123277\n",
      "   macro avg       0.78      0.51      0.50    123277\n",
      "weighted avg       0.94      0.95      0.93    123277\n",
      "\n",
      "Confusion Matrix: [[117360     45]\n",
      " [  5801     71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('lda', LDA(n_components=1)),  # LDA transformation\n",
    "    ('rf', RandomForestClassifier(max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50, random_state=42))  # Random Forest classifier\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Test the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\", report)\n",
    "print(\"Confusion Matrix:\", conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf73da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
